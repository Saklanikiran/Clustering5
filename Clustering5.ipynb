{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ae8c827-9232-4fb4-abc9-f249b507faed",
   "metadata": {},
   "source": [
    "# Ans : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6443cd1-88bf-4252-8289-9aa8f3cacec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Contingency Matrix: Also known as a confusion matrix, it is a table used to describe the performance of a classification model. It compares \n",
    "    the actual labels with the predicted labels.\n",
    "Usage: It helps in evaluating metrics like accuracy, precision, recall, and F1-score by providing counts of true positives (TP), false \n",
    "    positives (FP), true negatives (TN), and false negatives (FN).\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c975f6-386a-49a1-98c9-8bf22ebea499",
   "metadata": {},
   "source": [
    "# Ans : 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91554d2-cbbd-40ab-a779-22aef035ae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pair Confusion Matrix: Specifically used for pairwise comparisons, often in the context of ranking or recommendation systems.\n",
    "Difference: While a regular confusion matrix compares single predicted labels to actual labels, a pair confusion matrix evaluates pairs \n",
    "    of items, checking if the relative order between pairs is correctly predicted.\n",
    "Usefulness: It is useful in scenarios where the order of predictions is more important than the absolute predictions, such as ranking tasks.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2157106-2f24-487c-8194-e1967e5dae33",
   "metadata": {},
   "source": [
    "# Ans : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818e9f02-f612-4d86-9996-8adae2ffad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Extrinsic Measure: These are performance metrics that evaluate a model based on its effectiveness in real-world tasks, such as machine translation, sentiment analysis, or question answering.\n",
    "Usage: Language models are evaluated by how well they perform on downstream tasks, often involving benchmarks like BLEU for translation or F1-score for question answering.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbdcfdb-7b30-4949-87ea-b2da88388fe3",
   "metadata": {},
   "source": [
    "# Ans : 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d6b2b0-973e-42d5-84a8-29895cf6f2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Intrinsic Measure: These are performance metrics that evaluate the model based on its own output, without regard to specific tasks, such as \n",
    "    perplexity in language models or clustering quality metrics like silhouette score.\n",
    "Difference: Intrinsic measures assess the model's direct performance (e.g., language fluency), while extrinsic measures assess its performance in applied tasks.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3ea5ca-406f-4f47-9cd2-507a4970f0dd",
   "metadata": {},
   "source": [
    "# Ans : 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bd06d5-6f43-4a0e-afd2-aed603aafe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Purpose: To provide a detailed breakdown of the model's performance by showing the counts of true positives, false positives, true negatives,\n",
    "    and false negatives.\n",
    "Identifying Strengths and Weaknesses: By examining where the model makes correct predictions versus errors, one can identify if the model \n",
    "    is biased towards certain classes or if it struggles with particular types of predictions.\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b92a3c-b9bf-4e19-94d6-db22fae0f946",
   "metadata": {},
   "source": [
    "# Ans : 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6c1d61-4ee2-405a-a0fa-7eae3b8ad5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c708762e-55c1-480c-80fd-62b4f3f541b2",
   "metadata": {},
   "source": [
    "# Ans : 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37095b9c-ec25-4c61-82b0-713d94ced7d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
